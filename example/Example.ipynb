{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "perfume_df = pd.read_csv('data/final_perfume_data.csv', encoding='unicode_escape')\n",
    "perfume_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.9)\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe a pefume with the following notes: {notes}\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "notes = \"floral, citrus, musk\"\n",
    "chain.run(notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Sequential Chain\n",
    "\n",
    "The output of the first chain is the input of the second chain, and so on. This works well for a single input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.9)\n",
    "\n",
    "# prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe a pefume with the following notes: {notes}\",\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "chain1 = LLMChain(llm=llm, prompt=first_prompt)\n",
    "# prompt template 2\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 word description for a perfume with the following name: {perfume_name}\",\n",
    ")\n",
    "\n",
    "# Chain 2\n",
    "chain2 = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "overall_simple_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "overall_simple_chain.run(notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Chain\n",
    "\n",
    "Works well for multiple inputs and outputs. The output of the first chain is the input of the second chain, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.9)\n",
    "\n",
    "# prompt template 1: give notes a name\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe a pefume with the following notes: {notes}\",\n",
    ")\n",
    "\n",
    "# Chain 1: input = notes, output = perfume name\n",
    "chain1 = LLMChain(llm=llm, prompt=first_prompt, output_key=\"perfume_name\")\n",
    "# prompt template 2: give name a description\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 word description for a perfume with the following name: {perfume_name}\",\n",
    ")\n",
    "\n",
    "# Chain 2: input = perfume name, output = perfume description\n",
    "chain2 = LLMChain(llm=llm, prompt=second_prompt, output_key=\"perfume_description\")\n",
    "#prompt template 3: convert the description to a tagline\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a catchy 8 word tagline for a perfume with the following description: {perfume_description}\",\n",
    ")\n",
    "\n",
    "# Chain 3: input = perfume description, output = perfume tagline\n",
    "chain3 = LLMChain(llm=llm, prompt=third_prompt, output_key=\"perfume_tagline\")\n",
    "# prompt template 4: similar fragrance to notes\n",
    "fifth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"List 3 other perfumes with the following notes: {notes}\",\n",
    ")\n",
    "\n",
    "# Chain 4: input = notes, output = similar fragrances\n",
    "chain4 = LLMChain(llm=llm, prompt=fifth_prompt, output_key=\"similar_fragrances\")\n",
    "# overall chain: input= notes\n",
    "# and output = perfume name, description, tagline, similar fragrances\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3, chain4],\n",
    "    input_variables=[\"notes\"],\n",
    "    output_variables=[\"perfume_name\", \"perfume_description\", \"perfume_tagline\", \"similar_fragrances\"],\n",
    "    verbose=True\n",
    ")\n",
    "notes = notes\n",
    "overall_chain(notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A with custom data with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings # chat completion model and embeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter # split text into characters\n",
    "from langchain.document_loaders import CSVLoader # load data from a csv file - from langchain_community\n",
    "from langchain.vectorstores import DocArrayInMemorySearch #vectorstore, in memory, no need to connect to an external vectorestore\n",
    "from IPython.display import display, Markdown  # display markdown in jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a document loader - each doc represents a perfume\n",
    "documents = 'data/final_perfume_data.csv'\n",
    "loader = CSVLoader(file_path=documents, encoding='unicode_escape').load()\n",
    "\n",
    "# split text into characters\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(loader)\n",
    "\n",
    "# create embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embed = embeddings.embed_query(\"floral, citrus, musk\") # example embedding\n",
    "print(len(embed)) #print the length of the embedding for the example \n",
    "print(embed[:5]) #print the numerical representation of the first 5 elements of the embedding\n",
    "\n",
    "# create vectorstore\n",
    "vectorstore_db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity search\n",
    "\n",
    "query = \"please list similar fragrances to chanel no 5.\"\n",
    "docs = vectorstore_db.similarity_search(query)\n",
    "\n",
    "results = \"\".join([docs[i].page_content for i in range(len(docs))])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## doing question answering - DO NOT DO THIS SECTION\n",
    "\n",
    "# create a retriever\n",
    "retriever = vectorstore_db.as_retriever()\n",
    "\n",
    "# create a chat model\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "\n",
    "# join docs in a variable\n",
    "# qdocs = \"\\n\\n\".join([docs[i].page_content for i in range(len(docs))])\n",
    "\n",
    "def qdocs(docs):\n",
    "    return \"\\n\\n\".join([docs[i].page_content for i in range(len(docs))])\n",
    "\n",
    "# get a response\n",
    "response = llm.invoke(f\"{qdocs} Question: {query}\")\n",
    "\n",
    "# display the response\n",
    "# display(Markdown(response.content))\n",
    "\n",
    "def display_markdown(content):\n",
    "    display(Markdown(content))\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieval\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Return the answer in a markdown table with the name, brand and image.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | qdocs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | display_markdown\n",
    ")\n",
    "\n",
    "# rag_chain.invoke(\"what are the notes of Freeworld Parfum?\")\n",
    "rag_chain.invoke(\"List 3 similar fragrances to Circus Fantasy.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
